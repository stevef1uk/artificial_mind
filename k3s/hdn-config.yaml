apiVersion: v1
kind: ConfigMap
metadata:
  name: hdn-config
  namespace: agi
data:
  config.json: |
    {
      "llm_provider": "openai",
      "llm_api_key": "",
      "mcp_endpoint": "http://hdn-server-rpi58.agi.svc.cluster.local:8080/mcp",
      "settings": {
        "model": "gemma-3-1b-it-q4_k_m.gguf",
        "temperature": "0.7",
        "max_tokens": "1000",
        "llm_timeout_seconds": "120",
        "openai_url": "http://llama-server.agi.svc.cluster.local:8085"
      },
      "server": {
        "port": 8080,
        "host": "0.0.0.0"
      }
    }
  domain.json: |
    {
      "methods": [
        {
          "task": "RespondToQuery",
          "preconditions": [],
          "subtasks": ["GenerateResponse"]
        },
        {
          "task": "HelpUser",
          "preconditions": [],
          "subtasks": ["GenerateResponse"]
        },
        {
          "task": "ExplainSystem",
          "preconditions": [],
          "subtasks": ["GenerateResponse"]
        },
        {
          "task": "ScrapeWebsite",
          "preconditions": [],
          "subtasks": ["ExecuteScraping", "ProcessScrapedData"]
        },
        {
          "task": "FetchData",
          "preconditions": [],
          "subtasks": ["ExecuteScraping", "ProcessScrapedData"]
        }
      ],
      "actions": [
        {
          "task": "GenerateResponse",
          "preconditions": [],
          "effects": ["response_generated"]
        },
        {
          "task": "ExecuteScraping",
          "preconditions": [],
          "effects": ["data_scraped"]
        },
        {
          "task": "ProcessScrapedData",
          "preconditions": ["data_scraped"],
          "effects": ["data_processed"]
        }
      ]
    }
  agents.yaml: |
    agents:
        - id: email_monitor_agent
          name: Email Monitor Agent
          description: Monitors emails and takes actions based on content
          role: Email Monitoring Specialist
          goal: Monitor inbox for important emails and notify or take actions
          backstory: |
            You are an email monitoring specialist that watches for important messages,
            categorizes them, and takes appropriate actions like flagging urgent items
            or summarizing daily digests.
          tools:
            - mcp_read_google_data
            - mcp_save_episode
            - tool_http_get
          capabilities:
            max_iterations: 10
            verbose: true
          triggers:
            schedule:
                - cron: '*/30 * * * *'
                  action: check_emails
            events:
                - type: user_request
                  keywords:
                    - check emails
                    - monitor inbox
                    - email summary
                - type: goal
                  goal_type: monitor_emails
          behavior:
            thinking_mode: true
            max_retries: 3
            response_format: structured
            use_memory: true
            memory_window: 24h
            prefer_tools: true
            tool_timeout: 60s
          tasks:
            - id: check_unread_emails
              description: Check for unread emails and categorize them
              expected_output: List of unread emails with categories
              tools:
                - mcp_read_google_data
              parameters:
                limit: 50
                query: unread
                type: email
        - id: website_status_monitor
          name: Website Status Monitor
          description: Checks if important websites are up and running, reports their HTTP status codes, and alerts if any are down. Perfect for monitoring critical services or your own infrastructure.
          role: Website Health Monitor
          goal: Monitor a list of websites and report their status (up/down, response times, HTTP status codes) via Telegram
          backstory: |
            You are a website monitoring specialist that checks the health of websites by making HTTP requests
            and analyzing their responses. You provide clear status reports and identify any issues, then notify
            via Telegram for immediate visibility.
          tools:
            - tool_http_get
            - tool_telegram_send
          capabilities:
            max_iterations: 10
            verbose: true
          triggers:
            schedule:
                - cron: '*/15 * * * *'
                  action: execute
          behavior:
            thinking_mode: true
            max_retries: 3
            response_format: structured
            use_memory: true
            memory_window: 24h
            prefer_tools: true
            tool_timeout: 60s
          tasks:
            - id: monitor_task
              description: Monitor and check status of websites
              expected_output: Status report for each website (up/down, HTTP status, response time)
              tools:
                - tool_http_get
              parameters:
                websites:
                    - https://me.sjfisher.com
                    - https://k3s.sjfisher.com
        - id: scraper_agent
          name: Scraper Agent
          description: Intelligent scraper that can plan and execute complex web extractions
          role: Web Scraping Specialist
          goal: Plan and execute web scraping operations to extract specific data from any given URL
          backstory: |
            You are an expert web scraper that uses Playwright and intelligent planning
            to navigate complex websites, handle autocompletes, and extract data accurately.
          tools:
            - smart_scrape
            - tool_http_get
          capabilities:
            max_iterations: 15
            verbose: true
          triggers:
            events:
              - type: user_request
                keywords:
                  - scrape
                  - extract data
                  - find price
                  - flight emissions
              - type: goal
                goal_type: scrape_web
          behavior:
            thinking_mode: true
            max_retries: 5
            use_memory: true
            prefer_tools: true
            tool_timeout: 120s
    
        - id: price_monitor_agent
          name: Price Monitor Agent
          description: Monitors product prices twice daily and alerts on Telegram if prices drop by more than 10%.
          role: Price Watcher & Analyst
          goal: Track a list of product URLs, extract prices, compare with history, and notify on significant price drops.
          backstory: |
            You are a dedicated shopping assistant. You check prices for a specific list of items twice a day.
            You maintain a historical record of prices in 'data/price_history.json'.
            Whenever you run, you compare the current price with the previous one.
            If a price falls by 10% or more, you send an urgent alert via Telegram.
          tools:
            - mcp_smart_scrape
            - tool_telegram_send
            - tool_file_read
            - tool_file_write
          capabilities:
            max_iterations: 15
            verbose: true
          triggers:
            schedule:
                - cron: '0 9,17 * * *'
                  action: "Check all watched products for price drops."
          behavior:
            thinking_mode: true
            max_retries: 3
            use_memory: true
            prefer_tools: true
          tasks:
            - id: price_monitor_flow
              description: "Scrape the current price of the ASUS Ascent laptop from Amazon, compare with history, and alert if changed."
              expected_output: "The current price of the laptop and alert status."
              tools:
                - mcp_smart_scrape
              parameters:
                url: "https://www.amazon.fr/-/en/gp/product/B0G1CC2949"
                goal: "extract the price"
                monitoring:
                  type: "value_change"
                  field: "price"
                  history_path: "config/price_history.json"
                  product_name: "ASUS Ascent Laptop"
                typescript_config: "import { test, expect } from '@playwright/test';\ntest('test', async ({ page }) => {\n  await page.goto('https://www.amazon.fr/-/en/gp/product/B0G1CC2949');\n  await page.click('#sp-cc-accept');\n});"
                get_html: true
