apiVersion: v1
kind: Secret
metadata:
  name: llm-config
  namespace: agi
type: Opaque
stringData:
  # LLM Provider: "openai", "anthropic", "local", or "ollama"
  LLM_PROVIDER: "openai"
  
  # Model name (must match what's loaded in your LLM server)
  LLM_MODEL: "gemma-3-1b-it-q4_k_m.gguf"
  
  # Base URL for OpenAI-compatible servers (e.g., llama.cpp)
  # For OpenAI: https://api.openai.com
  # For local llama.cpp: http://llama-server.agi.svc.cluster.local:8085
  # For Ollama: http://ollama.agi.svc.cluster.local:11434
  OPENAI_BASE_URL: "http://llama-server.agi.svc.cluster.local:8085"
  
  # Alternative: OLLAMA_BASE_URL (for Ollama servers)
  # Uncomment and set this if using Ollama instead of OpenAI-compatible server
  # OLLAMA_BASE_URL: "http://ollama.agi.svc.cluster.local:11434"
  
  # Optional: API key (if using OpenAI/Anthropic)
  # LLM_API_KEY: ""
  
  # Timeout settings
  LLM_TIMEOUT: "120s"
  LLM_TIMEOUT_SECONDS: "120"

