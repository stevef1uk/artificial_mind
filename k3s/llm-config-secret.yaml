apiVersion: v1
kind: Secret
metadata:
  name: llm-config
  namespace: agi
type: Opaque
stringData:
  # LLM Provider: "openai", "anthropic", "local", or "ollama"
  LLM_PROVIDER: "ollama"
  
  # Model name (must match what's loaded in your LLM server)
  LLM_MODEL: "llama3.2:latest"
  
  # Base URL for OpenAI-compatible servers (e.g., llama.cpp)
  # For OpenAI: https://api.openai.com
  # For local llama.cpp: http://llama-server.agi.svc.cluster.local:8085
  # For Ollama: http://ollama.agi.svc.cluster.local:11434
  OPENAI_BASE_URL: "http://192.168.1.53:11434"
  
  # Alternative: OLLAMA_BASE_URL (for Ollama servers)
  # Base URL for Ollama (without /api/chat or /api/generate)
  OLLAMA_BASE_URL: "http://192.168.1.53:11434"
  
  # OLLAMA_URL: Full URL with API path (for tools that need it)
  # For llama.cpp (OpenAI-compatible): use /v1/chat/completions endpoint
  # For Ollama: use /api/chat endpoint
  OLLAMA_URL: "http://192.168.1.53:11434/api/chat"
  
  # LLM_ENDPOINT: Alternative name for LLM endpoint (used by wiki-summarizer)
  # For llama.cpp (OpenAI-compatible): use /v1/chat/completions endpoint
  # For Ollama: use /api/generate endpoint
  LLM_ENDPOINT: "http://192.168.1.53:11434/api/generate"
  
  # Optional: API key (if using OpenAI/Anthropic)
  # LLM_API_KEY: ""
  
  # Timeout settings
  LLM_TIMEOUT: "120s"
  LLM_TIMEOUT_SECONDS: "120"
  
  # Async LLM Queue Configuration
  USE_ASYNC_LLM_QUEUE: "1"
  ASYNC_LLM_MAX_WORKERS: "3"
  ASYNC_LLM_TIMEOUT_SECONDS: "60"
  
  # GPU Optimization: LLM request throttling (prevents GPU overload)
  LLM_MAX_CONCURRENT_REQUESTS: "2"
  
  # Backpressure: Queue size limits to prevent backlog
  # High-priority requests (user chat/chain-of-thought) - allow more
  LLM_MAX_HIGH_PRIORITY_QUEUE: "100"
  # Low-priority requests (background tasks) - limit to prevent backlog
  LLM_MAX_LOW_PRIORITY_QUEUE: "50"
  
  # Async HTTP Queue Configuration (for FSM)
  USE_ASYNC_HTTP_QUEUE: "1"
  FSM_MAX_CONCURRENT_HTTP_REQUESTS: "5"
  # Increased timeout for LLM calls (concept extraction, goal screening, etc.)
  # LLM calls can take 30-120 seconds when GPU is busy
  FSM_HTTP_TIMEOUT_SECONDS: "120"
  
  # External Integrations
  # N8N_WEBHOOK_URL: "https://k3s.sjfisher.com/webhook/..." # Set this manually using kubectl patch or edit before applying

