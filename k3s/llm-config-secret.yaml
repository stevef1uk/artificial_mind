apiVersion: v1
kind: Secret
metadata:
  name: llm-config
  namespace: agi
type: Opaque
stringData:
  # LLM Provider: "openai", "anthropic", "local", or "ollama"
  LLM_PROVIDER: "openai"
  
  # Model name (must match what's loaded in your LLM server)
  LLM_MODEL: "gemma-3-1b-it-q4_k_m.gguf"
  
  # Base URL for OpenAI-compatible servers (e.g., llama.cpp)
  # For OpenAI: https://api.openai.com
  # For local llama.cpp: http://llama-server.agi.svc.cluster.local:8085
  # For Ollama: http://ollama.agi.svc.cluster.local:11434
  OPENAI_BASE_URL: "http://llama-server.agi.svc.cluster.local:8085"
  
  # Alternative: OLLAMA_BASE_URL (for Ollama servers)
  # Base URL for Ollama (without /api/chat or /api/generate)
  OLLAMA_BASE_URL: "http://ollama.agi.svc.cluster.local:11434"
  
  # OLLAMA_URL: Full URL with API path (for tools that need it)
  # For llama.cpp (OpenAI-compatible): use /v1/chat/completions endpoint
  # For Ollama: use /api/chat endpoint
  OLLAMA_URL: "http://llama-server.agi.svc.cluster.local:8085/v1/chat/completions"
  
  # LLM_ENDPOINT: Alternative name for LLM endpoint (used by wiki-summarizer)
  # For llama.cpp (OpenAI-compatible): use /v1/chat/completions endpoint
  # For Ollama: use /api/generate endpoint
  LLM_ENDPOINT: "http://llama-server.agi.svc.cluster.local:8085/v1/chat/completions"
  
  # Optional: API key (if using OpenAI/Anthropic)
  # LLM_API_KEY: ""
  
  # Timeout settings
  LLM_TIMEOUT: "120s"
  LLM_TIMEOUT_SECONDS: "120"
  
  # Async LLM Queue Configuration
  USE_ASYNC_LLM_QUEUE: "1"
  ASYNC_LLM_MAX_WORKERS: "3"
  ASYNC_LLM_TIMEOUT_SECONDS: "60"
  
  # GPU Optimization: LLM request throttling (prevents GPU overload)
  LLM_MAX_CONCURRENT_REQUESTS: "2"
  
  # Async HTTP Queue Configuration (for FSM)
  USE_ASYNC_HTTP_QUEUE: "1"
  FSM_MAX_CONCURRENT_HTTP_REQUESTS: "5"
  FSM_HTTP_TIMEOUT_SECONDS: "30"

