{
  "llm_provider": "openai",
  "llm_api_key": "not-needed",
  "mcp_endpoint": "mock://localhost:3000/mcp",
  "settings": {
    "model": "qwen2.5-coder:7b",
    "temperature": "0.7",
    "max_tokens": "4096",
    "llm_timeout_seconds": "600",
    "openai_url": "http://localhost:11434"
  },
  "server": {
    "port": 8081,
    "host": "0.0.0.0"
  }
}
