# =============================================================================
# AGI System Environment Configuration
# =============================================================================
# This file contains example values for all environment variables used
# by the Artificial Mind system. Copy this to .env and update with your
# actual values. Never commit .env to version control.
# =============================================================================

# -----------------------------------------------------------------------------
# System Control Flags
# -----------------------------------------------------------------------------
# DISABLE_NEWS_POLLER: Set to true to disable automatic news ingestion
#   - false: News poller runs automatically (default)
#   - true: News poller is disabled
DISABLE_NEWS_POLLER=false

# ALLOW_REQUESTS: Controls whether the system accepts external requests
#   - true: System accepts requests (default)
#   - false: System rejects all external requests
ALLOW_REQUESTS=true

# -----------------------------------------------------------------------------
# MCP (Model Context Protocol) Configuration
# -----------------------------------------------------------------------------
# MCP_ENDPOINT: URL endpoint for MCP server communication
#   Format: http://hostname:port/mcp
#   Used for model context protocol interactions
MCP_ENDPOINT=http://localhost:8081/mcp

# -----------------------------------------------------------------------------
# Monitor Configuration
# -----------------------------------------------------------------------------
# MONITOR_LLM_WORKER_TIMEOUT_SECONDS: Timeout for LLM worker threads in monitor
#   - Time in seconds before a worker thread times out
#   - Increase if you have slow LLM responses
#   - Default: 600 (10 minutes) for slower models, 120 for faster ones
MONITOR_LLM_WORKER_TIMEOUT_SECONDS=600

# MONITOR_STATIC_DIR: Path to static files directory for monitor UI
#   - Absolute path to static assets (HTML, CSS, JS)
#   - If commented out, uses default: monitor/static relative to executable
# MONITOR_STATIC_DIR=/path/to/monitor/static

# -----------------------------------------------------------------------------
# Execution Method Configuration
# -----------------------------------------------------------------------------
# EXECUTION_METHOD: How code execution is performed
#   - docker: Execute code in Docker containers (recommended for x86)
#   - drone: Execute code via Drone CI (for ARM64/remote execution)
export EXECUTION_METHOD=docker

# ENABLE_ARM64_TOOLS: Enable ARM64-specific tool compilation
#   - false: Disable ARM64 tools (use on x86 systems)
#   - true: Enable ARM64 tools (use on ARM systems like Raspberry Pi)
ENABLE_ARM64_TOOLS=false

# -----------------------------------------------------------------------------
# Remote Execution Configuration (Drone CI) - Only needed if using an RPI K8s cluster
# -----------------------------------------------------------------------------
# RPI_HOST: IP address or hostname of remote Raspberry Pi for execution
#   - Used when EXECUTION_METHOD=drone
#   - Example: 192.168.1.63
# RPI_HOST=192.168.1.63

# DRONE_TOKEN: Authentication token for Drone CI API
#   - Required for remote execution via Drone
#   - Get from your Drone CI server settings
#   - WARNING: Replace with your actual token in .env file
#nDRONE_TOKEN=your-drone-token-here

# DRONE_REPO: Repository identifier for Drone CI
#   - Format: username/repository-name
#   - Example: stevef/agi-runtime
# DRONE_REPO=stevef/agi-runtime

# -----------------------------------------------------------------------------
# Vector Database Configuration
# -----------------------------------------------------------------------------
# WEAVIATE_URL: URL for Weaviate vector database
#   - Used for semantic search and vector storage
#   - Default: http://localhost:8080
WEAVIATE_URL=http://localhost:8080

# QDRANT_URL: URL for Qdrant vector database (alternative to Weaviate)
#   - Used for vector similarity search
#   - Default: http://localhost:6333
QDRANT_URL=http://localhost:6333

# -----------------------------------------------------------------------------
# FSM (Finite State Machine) Configuration
# -----------------------------------------------------------------------------
# FSM_AGENT_ID: Unique identifier for this FSM agent instance
#   - Used to distinguish multiple agent instances
#   - Default: agent_1
FSM_AGENT_ID=agent_1

# FSM_AUTONOMY: Enable autonomous goal execution by FSM
#   - true: FSM can autonomously create and execute goals
#   - false: FSM requires manual approval for goals
FSM_AUTONOMY=true

# FSM_AUTONOMY_EVERY: Interval in seconds between autonomous goal checks
#   - How often FSM checks for new autonomous goals
#   - Lower values = more frequent checks (more resource intensive)
#   - Default: 180 (3 minutes), conservative: 60 (1 minute)
FSM_AUTONOMY_EVERY=180

# FSM_MAX_ACTIVE_GOALS: Maximum number of active goals FSM can manage
#   - Limits concurrent goal execution to prevent resource overload
#   - Adjust based on system resources (CPU, memory, GPU)
#   - Default: 8 (for 16-core systems), conservative: 2
FSM_MAX_ACTIVE_GOALS=8

# FSM_MAX_CONCURRENT_HYP_TESTS: Maximum concurrent hypothesis tests
#   - Limits parallel hypothesis testing operations
#   - Lower values reduce GPU/CPU load
#   - Default: 4 (for 16-core systems), conservative: 1
FSM_MAX_CONCURRENT_HYP_TESTS=4

# -----------------------------------------------------------------------------
# Service URLs
# -----------------------------------------------------------------------------
# HDN_URL: URL for HDN (Hypothesis Development Network) server
#   - Handles code generation and execution
#   - Default: http://localhost:8081
HDN_URL=http://localhost:8081

# PRINCIPLES_URL: URL for Principles server
#   - Manages system principles and ethical guidelines
#   - Default: http://localhost:8084
PRINCIPLES_URL=http://localhost:8084

# NATS_URL: URL for NATS message bus
#   - Used for inter-service communication
#   - Format: nats://hostname:port
#   - Default: nats://127.0.0.1:4222
NATS_URL=nats://127.0.0.1:4222

# REDIS_URL: URL for Redis cache/database
#   - Used for caching and temporary data storage
#   - Format: redis://hostname:port
#   - Default: redis://localhost:6379
export REDIS_URL=redis://localhost:6379

# GOAL_MANAGER_URL: URL for Goal Manager service
#   - Manages system goals and objectives
#   - Default: http://localhost:8090
GOAL_MANAGER_URL=http://localhost:8090

# -----------------------------------------------------------------------------
# Graph Database Configuration (Neo4j)
# -----------------------------------------------------------------------------
# NEO4J_URI: Connection URI for Neo4j graph database
#   - Format: bolt://hostname:port
#   - Default: bolt://localhost:7687
NEO4J_URI=bolt://localhost:7687

# NEO4J_USER: Username for Neo4j authentication
#   - Default: neo4j
NEO4J_USER=neo4j

# NEO4J_PASS: Password for Neo4j authentication
#   - Change from default in production!
#   - Default: test1234
NEO4J_PASS=test1234

# -----------------------------------------------------------------------------
# LLM (Large Language Model) Configuration
# -----------------------------------------------------------------------------
# LLM_PROVIDER: Which LLM provider to use
#   - openai: OpenAI API (requires OPENAI_API_KEY and OPENAI_BASE_URL)
#   - anthropic: Anthropic Claude API (requires ANTHROPIC_API_KEY)
#   - local: Local Ollama instance (requires OLLAMA_BASE_URL)
#   - ollama: Alias for local
#   - mock: Mock provider for testing
export LLM_PROVIDER=openai

# LLM_MODEL: Model name/identifier to use
#   - Format depends on provider:
#     * OpenAI: gpt-4, gpt-3.5-turbo, etc.
#     * Anthropic: claude-3-sonnet-20240229, etc.
#     * Ollama: gemma3:latest, llama2, Qwen2.5-VL-7B-Instruct:latest, etc.
#   - For local models, include full path if needed: gemma-3-1b-it-q4_k_m.gguf
export LLM_MODEL=gemma-3-1b-it-q4_k_m.gguf

# LLM_TIMEOUT: Maximum time to wait for LLM response
#   - Format: number followed by 's' (seconds) or 'm' (minutes)
#   - Examples: 120s, 5m, 600s
#   - Increase for slow models or large requests
#   - Default: 600s (10 minutes) for slower models, 120s for faster ones
export LLM_TIMEOUT=600s

# OLLAMA_BASE_URL: Base URL for Ollama API (when using local/ollama provider)
#   - Format: http://hostname:port or http://hostname:port/api/chat
#   - Default: http://localhost:11434
#   - Can point to remote Ollama instance
#   - Example: http://192.168.1.45:11434/api/chat or http://192.168.1.45:8085
OLLAMA_BASE_URL=http://192.168.1.45:8085

# OPENAI_BASE_URL: Base URL for OpenAI-compatible API
#   - Used when LLM_PROVIDER=openai
#   - Can point to OpenAI API or compatible proxy (e.g., vLLM, llama.cpp server)
#   - Default OpenAI: https://api.openai.com/v1
#   - For local proxies: http://localhost:8085
OPENAI_BASE_URL=http://localhost:8085

# OPENAI_API_KEY: API key for OpenAI (required if using OpenAI provider)
#   - Get from https://platform.openai.com/api-keys
#   - Keep this secret! Never commit to version control.
# OPENAI_API_KEY=sk-your-key-here

# ANTHROPIC_API_KEY: API key for Anthropic Claude (required if using Anthropic provider)
#   - Get from https://console.anthropic.com/
#   - Keep this secret! Never commit to version control.
# ANTHROPIC_API_KEY=sk-ant-your-key-here

# -----------------------------------------------------------------------------
# Bootstrap Configuration (Knowledge Graph Seeding)
# -----------------------------------------------------------------------------
# FSM_BOOTSTRAP_SEEDS: Comma-separated list of seed topics for knowledge graph
#   - Initial topics to bootstrap the knowledge graph
#   - Example: Science,Maths,Computing,Biology,Physics
FSM_BOOTSTRAP_SEEDS=Science,Maths,Computing,Biology,Physics

# FSM_BOOTSTRAP_MAX_DEPTH: Maximum depth for knowledge graph traversal
#   - How many levels deep to explore from seed topics
#   - Higher values = more comprehensive but slower
#   - Default: 1
FSM_BOOTSTRAP_MAX_DEPTH=1

# FSM_BOOTSTRAP_MAX_NODES: Maximum number of nodes to create in bootstrap
#   - Limits total nodes created during bootstrap phase
#   - Prevents excessive resource usage
#   - Default: 50
FSM_BOOTSTRAP_MAX_NODES=50

# FSM_BOOTSTRAP_RPM: Requests per minute for bootstrap operations
#   - Rate limiting for bootstrap API calls
#   - Lower values = slower but safer (prevents API rate limits)
#   - Default: 60 (conservative), can be increased to 120 for faster systems
FSM_BOOTSTRAP_RPM=120

# FSM_BOOTSTRAP_SEED_BATCH: Number of seed topics to process in parallel
#   - Batch size for parallel seed processing
#   - Higher values = faster but more resource intensive
#   - Default: 3 (conservative), can be increased to 6 for powerful systems
FSM_BOOTSTRAP_SEED_BATCH=6

# FSM_BOOTSTRAP_COOLDOWN_HOURS: Hours to wait between bootstrap runs
#   - Prevents excessive bootstrap operations
#   - Default: 3
FSM_BOOTSTRAP_COOLDOWN_HOURS=3

# -----------------------------------------------------------------------------
# Executor Configuration (Concurrency Limits)
# -----------------------------------------------------------------------------
# GOMAXPROCS: Maximum number of OS threads for Go runtime
#   - Controls Go's parallelism
#   - Set to number of CPU cores for optimal performance
#   - Default: 16 (for 16-core systems)
GOMAXPROCS=16

# HDN_MAX_CONCURRENT_EXECUTIONS: Maximum concurrent code executions in HDN
#   - Limits parallel code execution to prevent resource overload
#   - Lower values = safer but slower
#   - Default: 12 (for 16-core systems), conservative: 3 (reduce if GPU is overloaded)
HDN_MAX_CONCURRENT_EXECUTIONS=12

# MONITOR_MAX_CONCURRENT_GOALS: Maximum concurrent goals in monitor
#   - Limits parallel goal processing in monitor UI
#   - Default: 8 (for 16-core systems), conservative: 2
MONITOR_MAX_CONCURRENT_GOALS=8

# AUTO_EXECUTOR_MAX_CONCURRENT: Maximum concurrent auto-executor operations
#   - Limits parallel automatic execution tasks
#   - Default: 4, conservative: 2
AUTO_EXECUTOR_MAX_CONCURRENT=4

# LLM_MAX_CONCURRENT_REQUESTS: Maximum concurrent LLM API requests globally
#   - Critical for GPU/resource management
#   - Lower values prevent GPU overload
#   - Default: 2 (conservative, increase only if GPU can handle more)
#   - Note: This is a global limit across all services
LLM_MAX_CONCURRENT_REQUESTS=2

# -----------------------------------------------------------------------------
# Docker Resource Limits
# -----------------------------------------------------------------------------
# DOCKER_MEMORY_LIMIT: Maximum memory for Docker containers
#   - Format: number followed by 'g' (GB) or 'm' (MB)
#   - Examples: 4g, 2048m
#   - Default: 4g (for x86 systems)
DOCKER_MEMORY_LIMIT=4g

# DOCKER_CPU_LIMIT: Maximum CPU allocation for Docker containers
#   - Number of CPU cores (can be fractional, e.g., 8.0, 4.5)
#   - Default: 8.0 (for x86 systems)
DOCKER_CPU_LIMIT=8.0

# DOCKER_PIDS_LIMIT: Maximum number of process IDs per container
#   - Prevents fork bombs and resource exhaustion
#   - Default: 1024
DOCKER_PIDS_LIMIT=1024

# DOCKER_TMPFS_SIZE: Size of tmpfs (temporary filesystem) for containers
#   - Format: number followed by 'g' (GB) or 'm' (MB)
#   - Used for temporary file storage in containers
#   - Default: 512m
DOCKER_TMPFS_SIZE=512m
